{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> em { font-style: normal; display: inherit; text-align: center; font-size: 90%; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style> em { font-style: normal; display: inherit; text-align: center; font-size: 90%; }</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning\n",
    "\n",
    "> Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.\n",
    "\n",
    "## What is a (Neural Network) NN?\n",
    "\n",
    "1. Single neuron == linear regression\n",
    "2. Simple NN graph:\n",
    "    ![simple_nn](Images/Others/01.jpg)\n",
    "    *Source: tutorialspoint*\n",
    "\n",
    "3. RELU stands for rectified linear unit is the most popular activation function right now that makes deep NNs train faster now.\n",
    "4. Hidden layers predicts connection between inputs automatically, thats what deep learning is good at.\n",
    "5. Deep NN consists of more hidden layers (Deeper layers):\n",
    "    ![deep_nn](Images/Others/02.png)\n",
    "    *Source: opennn.net*\n",
    "    \n",
    "6. Each Input will be connected to the hidden layer and the NN will decide the connections.\n",
    "7. Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning with neural networks\n",
    "\n",
    "1. Different types of neural networks for supervised learning which includes:\n",
    "  - CNN or convolutional neural networks (Useful in computer vision)\n",
    "  - RNN or Recurrent neural networks (Useful in Speech recognition or NLP)\n",
    "  - Standard NN (Useful for Structured data)\n",
    "  - Hybrid/custom NN or a Collection of NNs types\n",
    "2. Structured data is like the databases and tables.\n",
    "3. Unstructured data is like images, video, audio, and text.\n",
    "4. Structured data gives more money because companies relies on prediction on its big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is deep learning taking off?\n",
    "\n",
    "- Deep learning is taking off for 3 reasons:\n",
    "  1. **Data**:\n",
    "     * Using this image we can conclude:\n",
    "       ![](Images/11.png)\n",
    "       *Source: deeplearning.ai*\n",
    "       \n",
    "     * For small data NN can perform as Linear regression or SVM (Support vector machine)\n",
    "     * For big data a small NN is better that SVM\n",
    "     * For big data a big NN is better that a medium NN is better that small NN.\n",
    "     * Hopefully we have a lot of data because the world is using the computer a little bit more\n",
    "       - Mobiles\n",
    "       - IOT (Internet of things)\n",
    "  2. **Computation**:\n",
    "     * GPUs.\n",
    "     * Powerful CPUs.\n",
    "     * Distributed computing.\n",
    "     * ASICs\n",
    "  3. **Algorithm**:\n",
    "     * Creative algorithms has appeared that changed the way NN works.\n",
    "        - For example using RELU function is so much better than using SIGMOID function in training a NN because it helps with the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Objective : Binary classification\n",
    "\n",
    "- Logistic regression to make a binary classifier.\n",
    "  ![log](Images/Others/03.png)\n",
    "  *Source: http://3.bp.blogspot.com*\n",
    "  \n",
    "- If the current image contains a cat or not.\n",
    "- Here are some notations:\n",
    "  - `M is the number of training vectors`\n",
    "  - `Nx is the size of the input vector`\n",
    "  - `Ny is the size of the output vector`\n",
    "  - `X(1) is the first input vector`\n",
    "  - `Y(1) is the first output vector`\n",
    "  - `X = [x(1) x(2).. x(M)]`\n",
    "  - `Y = (y(1) y(2).. y(M))`\n",
    "- We will use python in this course.\n",
    "- In NumPy we can make matrices and make operations on them in a fast and reliable time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "- Algorithm is used for classification algorithm of 2 classes.\n",
    "- Equations:\n",
    "  - Simple equation:\t`y = wx + b`\n",
    "  - If x is a vector: `y = w(transpose)x + b`\n",
    "  - If we need y to be in between 0 and 1 (probability): `y = sigmoid(w(transpose)x + b)`\n",
    "  - In some notations this might be used: `y = sigmoid(w(transpose)x)`\n",
    "    - While `b` is `w0` of `w` and we add `x0 = 1`. but we won't use this notation in the course (Andrew said that the first notation is better).\n",
    "- In binary classification `Y` has to be between `0` and `1`.\n",
    "- In the last equation `w` is a vector of `Nx` and `b` is a real number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression cost function\n",
    "\n",
    "- First loss function would be the square root error:  `L(y',y) = 1/2 (y' - y)^2`\n",
    "  - But we won't use this notation because it leads us to optimization problem which is non convex, means it contains local optimum points.\n",
    "- This is the function that we will use: `L(y',y) = - (y*log(y') + (1-y)*log(1-y'))`\n",
    "- To explain the last function lets see:\n",
    "  - if `y = 1` ==> `L(y',1) = -log(y')`  ==> we want `y'` to be the largest   ==> `y`' biggest value is 1\n",
    "  - if `y = 0` ==> `L(y',0) = -log(1-y')` ==> we want `1-y'` to be the largest ==> `y'` to be smaller as possible because it can only has 1 value.\n",
    "- Then the Cost function will be: `J(w,b) = (1/m) * Sum(L(y'[i],y[i]))`\n",
    "- The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "- We want to predict `w` and `b` that minimize the cost function.\n",
    "- Our cost function is convex.\n",
    "- First we initialize `w` and `b` to 0,0 or initialize them to a random value in the convex function and then try to improve the values the reach minimum value.\n",
    "- In Logistic regression people always use 0,0 instead of random.\n",
    "- The gradient decent algorithm repeats: `w = w - alpha * dw`\n",
    "  where alpha is the learning rate and `dw` is the derivative of `w` (Change to `w`)\n",
    "  The derivative is also the slope of `w`\n",
    "- Looks like greedy algorithms. the derivative give us the direction to improve our parameters.\n",
    "\n",
    "- The actual equations we will implement:\n",
    "  - `w = w - alpha * d(J(w,b) / dw)`        (how much the function slopes in the w direction)\n",
    "  - `b = b - alpha * d(J(w,b) / db)`        (how much the function slopes in the d direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "- We will talk about some of required calculus.\n",
    "- You don't need to be a calculus geek to master deep learning but you'll need some skills from it.\n",
    "- Derivative of a linear line is its slope.\n",
    "  - ex. `f(a) = 3a`                    `d(f(a))/d(a) = 3`\n",
    "  - if `a = 2` then `f(a) = 6`\n",
    "  - if we move a a little bit `a = 2.001` then `f(a) = 6.003` means that we multiplied the derivative (Slope) to the moved area and added it to the last result.\n",
    "\n",
    "### More Derivatives examples\n",
    "\n",
    "- `f(a) = a^2`  ==> `d(f(a))/d(a) = 2a`\n",
    "  - `a = 2`  ==> `f(a) = 4`\n",
    "  - `a = 2.0001` ==> `f(a) = 4.0004` approx.\n",
    "- `f(a) = a^3`  ==> `d(f(a))/d(a) = 3a^2`\n",
    "- `f(a) = log(a)`  ==> `d(f(a))/d(a) = 1/a`\n",
    "- To conclude, Derivative is the slope and slope is different in different points in the function thats why the derivative is a function.\n",
    "\n",
    "### Computation graph\n",
    "\n",
    "- Its a graph that organizes the computation from left to right.\n",
    "  ![](Images/02.png)\n",
    "\n",
    "### Derivatives with a Computation Graph\n",
    "\n",
    "- Calculus chain rule says:\n",
    "  If `x -> y -> z`          (x effect y and y effects z)\n",
    "  Then `d(z)/d(x) = d(z)/d(y) * d(y)/d(x)`\n",
    "  ![](Images/03.png)\n",
    "- We compute the derivatives on a graph from right to left and it will be a lot more easier.\n",
    "- `dvar` means the derivatives of a final output variable with respect to various intermediate quantities.\n",
    "\n",
    "### Logistic Regression Gradient Descent\n",
    "\n",
    "- The derivatives of gradient decent example for one sample with two features `x1` and `x2`.\n",
    "  ![](Images/04.png)\n",
    "\n",
    "### Gradient Descent on m Examples\n",
    "\n",
    "- Lets say we have these variables:\n",
    "\n",
    "  ```\n",
    "  \tX1\t\t\t\t\tFeature\n",
    "  \tX2                  Feature\n",
    "  \tW1                  Weight of the first feature.\n",
    "  \tW2                  Weight of the second feature.\n",
    "  \tB                   Logistic Regression parameter.\n",
    "  \tM                   Number of training examples\n",
    "  \tY(i)\t\t\t\tExpected output of i\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural networks\n",
    "\n",
    "> Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.\n",
    "\n",
    "### Neural Networks Overview\n",
    "\n",
    "- In logistic regression we had:\n",
    "\n",
    "  ```\n",
    "  X1  \\  \n",
    "  X2   ==>  z = XW + B ==> a = Sigmoid(z) ==> l(a,Y)\n",
    "  X3  /\n",
    "  ```\n",
    "\n",
    "- In neural networks with one layer we will have:\n",
    "\n",
    "  ```\n",
    "  X1  \\  \n",
    "  X2   =>  z1 = XW1 + B1 => a1 = Sigmoid(z1) => z2 = a1W2 + B2 => a2 = Sigmoid(z2) => l(a2,Y)\n",
    "  X3  /\n",
    "  ```\n",
    "\n",
    "\n",
    "- `X` is the input vector `(X1, X2, X3)`, and `Y` is the output variable `(1x1)`\n",
    "- NN is stack of logistic regression objects.\n",
    "\n",
    "### Neural Network Representation\n",
    "\n",
    "- We will define the neural networks that has one hidden layer.\n",
    "- NN contains of input layers, hidden layers, output layers.\n",
    "- Hidden layer means we cant see that layers in the training set.\n",
    "- `a0 = x` (the input layer)\n",
    "- `a1` will represent the activation of the hidden neurons.\n",
    "- `a2` will represent the output layer.\n",
    "- We are talking about 2 layers NN. The input layer isn't counted.\n",
    "\n",
    "### Computing a Neural Network's Output\n",
    "\n",
    "- Equations of Hidden layers:\n",
    "  ![](Images/05.png)\n",
    "- Here are some informations about the last image:\n",
    "  - `noOfHiddenNeurons = 4`\n",
    "  - `Nx = 3`\n",
    "  - Shapes of the variables:\n",
    "    - `W1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,nx)`\n",
    "    - `b1` is the matrix of the first hidden layer, it has a shape of `(noOfHiddenNeurons,1)`\n",
    "    - `z1` is the result of the equation `z1 = W1*X + b`, it has a shape of `(noOfHiddenNeurons,1)`\n",
    "    - `a1` is the result of the equation `a1 = sigmoid(z1)`, it has a shape of `(noOfHiddenNeurons,1)`\n",
    "    - `W2` is the matrix of the second hidden layer, it has a shape of `(1,noOfHiddenNeurons)`\n",
    "    - `b2` is the matrix of the second hidden layer, it has a shape of `(1,1)`\n",
    "    - `z2` is the result of the equation `z2 = W2*a1 + b`, it has a shape of `(1,1)`\n",
    "    - `a2` is the result of the equation `a2 = sigmoid(z2)`, it has a shape of `(1,1)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
