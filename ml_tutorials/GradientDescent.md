## Gradient descent variants
1. Batch gradient descent
2. Stochastic gradient descent
3. Mini-batch gradient descent

## Challenges
1. **Gradient descent optimization algorithms**
   - Momentum
   - Nesterov accelerated gradient
   - Adagrad
   - Adadelta
   - RMSprop
   - Adam
   - AdaMax
   - Nadam
   - AMSGrad
2. **Visualization of algorithms**
3. **Which optimizer to choose?**
4. **Parallelizing and distributing SGD**
    - Hogwild!
    - Downpour SGD
    - Delay-tolerant Algorithms for SGD
    - TensorFlow
    - Elastic Averaging SGD

5. **Additional strategies for optimizing SGD**
    - Shuffling and Curriculum Learning
    - Batch normalization
    - Early Stopping
    - Gradient noise
6. **Conclusion**
7. **References**